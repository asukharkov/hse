{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0wBrTmoRW97n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.core import ObservationWrapper\n",
        "from gym.spaces import Box\n",
        "from scipy.misc import imresize\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import transform\n",
        "from keras.layers import Conv2D, Dense, Flatten, InputLayer\n",
        "import os\n",
        "from IPython import display\n",
        "import random\n",
        "from gym.core import Wrapper\n",
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "import keras\n",
        "import cv2\n",
        "import pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "alGDYTtiW98M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PreprocessAtari(ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        ObservationWrapper.__init__(self, env)\n",
        "        self.img_size = (64, 64)\n",
        "        self.observation_space = Box(0.0, 1.0, (self.img_size[0], self.img_size[1], 1))\n",
        "\n",
        "    def _observation(self, img):    \n",
        "        img = img[34 : -16, :, :] # croping\n",
        "        img = cv2.resize(img, self.img_size) # resizing\n",
        "        img = img.mean(-1, keepdims=True) # grayscaling\n",
        "        img = img.astype('float32') / 255. # converting to (0, 1) and float32\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dy-h4xsEW98a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FrameBuffer(Wrapper):\n",
        "    def __init__(self, env, n_frames=4, dim_order='tensorflow'):\n",
        "        super(FrameBuffer, self).__init__(env)\n",
        "        self.dim_order = dim_order\n",
        "        if dim_order == 'tensorflow':\n",
        "            height, width, n_channels = env.observation_space.shape\n",
        "            obs_shape = [height, width, n_channels * n_frames]\n",
        "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "        \n",
        "    def reset(self):\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "    \n",
        "    def step(self, action):\n",
        "        new_img, reward, done, info = self.env.step(action)\n",
        "        self.update_buffer(new_img)\n",
        "        return self.framebuffer, reward, done, info\n",
        "    \n",
        "    def update_buffer(self, img):\n",
        "        if self.dim_order == 'tensorflow':\n",
        "            offset = self.env.observation_space.shape[-1]\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goMnMaoSW98o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_env():\n",
        "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
        "    env = PreprocessAtari(env)\n",
        "    env = FrameBuffer(env, n_frames=4, dim_order='tensorflow')\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmapQpo6W98-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for _ in range(50):\n",
        "    obs, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "plt.title(\"Game image\")\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "plt.show()\n",
        "plt.title(\"Agent observation (4 frames left to right)\")\n",
        "plt.imshow(obs.transpose([0, 2, 1]).reshape([state_dim[0], -1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNMDE0voW99W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXuChKjXW99r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, name, state_shape, n_actions, epsilon=0, reuse=False):\n",
        "        \"\"\"A simple DQN agent\"\"\"\n",
        "        with tf.variable_scope(name, reuse=reuse):\n",
        "            self.network = keras.models.Sequential()\n",
        "            self.network.add(Conv2D(16, (3, 3), strides=2, activation='relu', input_shape=state_shape))\n",
        "            self.network.add(Conv2D(32, (3, 3), strides=2, activation='relu'))\n",
        "            self.network.add(Conv2D(64, (3, 3), strides=2, activation='relu'))\n",
        "            self.network.add(Flatten())\n",
        "            self.network.add(Dense(256, activation='relu'))\n",
        "            self.network.add(Dense(n_actions, activation='linear'))\n",
        "            self.state_t = tf.placeholder('float32', [None, ] + list(state_shape))\n",
        "            self.qvalues_t = self.get_symbolic_qvalues(self.state_t)\n",
        "        self.weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=name)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def get_symbolic_qvalues(self, state_t):\n",
        "        qvalues = self.network(state_t)\n",
        "        assert tf.is_numeric_tensor(qvalues) and qvalues.shape.ndims == 2, \\\n",
        "            \"please return 2d tf tensor of qvalues [you got %s]\" % repr(qvalues)\n",
        "        assert int(qvalues.shape[1]) == n_actions\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, state_t):\n",
        "        sess = tf.get_default_session()\n",
        "        return sess.run(self.qvalues_t, {self.state_t: state_t})\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "        should_explore = np.random.choice([0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fjFhHYCXW9-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(\"dqn_agent\", state_dim, n_actions, epsilon=0.99)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgpIqJ3bW9-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s = env.reset()\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(\n",
        "                axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, done, _ = env.step(action)\n",
        "            reward += r\n",
        "            if done:\n",
        "                break\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nm1GQ2FkW9-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size):\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._current = 0\n",
        "        self._flag = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
        "        data = (obs_t, action, reward, obs_tp1, done)\n",
        "        if self._flag:\n",
        "            self._storage[self._current] = data\n",
        "        else:\n",
        "            self._storage.append(data)\n",
        "        self._current += 1\n",
        "        if self._current == self._maxsize:\n",
        "            self._flag = 1\n",
        "        self._current %= self._maxsize\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxes = [random.randint(0, self.__len__() - 1) for _ in range(batch_size)]\n",
        "        obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = [], [], [], [], []\n",
        "        for i in idxes:\n",
        "            data = self._storage[i]\n",
        "            obs, act, rew, next_obs, done = data\n",
        "            obs_batch.append(np.array(obs, copy=False))\n",
        "            act_batch.append(np.array(act, copy=False))\n",
        "            rew_batch.append(rew)\n",
        "            next_obs_batch.append(np.array(next_obs, copy=False))\n",
        "            done_mask.append(done)\n",
        "        return np.array(obs_batch), np.array(act_batch), np.array(rew_batch), np.array(next_obs_batch), np.array(done_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zBo1iUGJW9-6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def play_and_record(agent, env, exp_replay, n_steps=1):\n",
        "    s = env.framebuffer\n",
        "    reward = 0\n",
        "    for i in range(n_steps):\n",
        "        qvalues = agent.get_qvalues([s])\n",
        "        action = agent.sample_actions(qvalues)[0]\n",
        "        next_s, done, rew, _ = env.step(action)\n",
        "        reward += rew\n",
        "        exp_replay.add(s, action, rew, next_s, done)\n",
        "        s = next_s\n",
        "        if done:\n",
        "            s = env.reset()\n",
        "    return reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YYZgDPRFW9_J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target_network = DQNAgent(\"target_network\", state_dim, n_actions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LFgkuBt1W9_Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_weigths_into_target_network(agent, target_network):\n",
        "    assigns = []\n",
        "    for w_agent, w_target in zip(agent.weights, target_network.weights):\n",
        "        assigns.append(tf.assign(w_target, w_agent, validate_shape=True))\n",
        "    return assigns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFORrJChW-AI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "copy_step = load_weigths_into_target_network(agent, target_network)\n",
        "sess.run(copy_step)\n",
        "sess.run([tf.assert_equal(w, w_target) for w, w_target in zip(agent.weights, target_network.weights)])\n",
        "print(\"It works!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1uq5MdJJW-AZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "actions_ph = tf.placeholder(tf.int32, shape=[None])\n",
        "rewards_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "next_obs_ph = tf.placeholder(tf.float32, shape=(None,) + state_dim)\n",
        "is_done_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "is_not_done = 1 - is_done_ph\n",
        "gamma = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxcKr9gwW-Ak",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "current_qvalues = agent.get_symbolic_qvalues(obs_ph)\n",
        "current_action_qvalues = tf.reduce_sum(tf.one_hot(actions_ph, n_actions) * current_qvalues, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vufzaIE6W-At",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "next_qvalues_target = target_network.get_symbolic_qvalues(next_obs_ph)\n",
        "next_state_values_target = tf.reduce_max(next_qvalues_target, axis=-1)\n",
        "reference_qvalues = rewards_ph + gamma * next_state_values_target * is_not_done\n",
        "td_loss = (current_action_qvalues - reference_qvalues) ** 2\n",
        "td_loss = tf.reduce_mean(td_loss)\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(td_loss, var_list=agent.weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FV-hYpkIW-A3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-rRPNZ7W-A-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#moving_average = lambda x, span=100, **kw: DataFrame(\n",
        "#    {'x': np.asarray(x)}).x.ewm(span=span, **kw).mean().values\n",
        "#%matplotlib inline\n",
        "\n",
        "mean_rw_history = []\n",
        "td_loss_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wX8NsZKXW-BE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "exp_replay = ReplayBuffer(100000)\n",
        "play_and_record(agent, env, exp_replay, n_steps=10000)\n",
        "\n",
        "def sample_batch(exp_replay, batch_size):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(batch_size)\n",
        "    return {\n",
        "        obs_ph: obs_batch, actions_ph: act_batch, rewards_ph: reward_batch,\n",
        "        next_obs_ph: next_obs_batch, is_done_ph: is_done_batch\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MpCe9gWrW-BL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in trange(60000):\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    if i % 500 == 0:\n",
        "        sess.run(copy_step)\n",
        "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(pandas.ewma(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whdChdQ8uYIx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in trange(60000):\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    if i % 500 == 0:\n",
        "        sess.run(copy_step)\n",
        "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(pandas.ewma(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EXPkK9mZuZpa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in trange(60000):\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    if i % 500 == 0:\n",
        "        sess.run(copy_step)\n",
        "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(pandas.ewma(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRyUa99ZuamG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in trange(60000):\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    if i % 500 == 0:\n",
        "        sess.run(copy_step)\n",
        "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(pandas.ewma(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyjStirPucTo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in trange(60000):\n",
        "    play_and_record(agent, env, exp_replay, 10)\n",
        "    _, loss_t = sess.run([train_step, td_loss], sample_batch(exp_replay, batch_size=64))\n",
        "    td_loss_history.append(loss_t)\n",
        "    if i % 500 == 0:\n",
        "        sess.run(copy_step)\n",
        "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
        "        mean_rw_history.append(evaluate(make_env(), agent, n_games=3))\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        print(\"buffer size = %i, epsilon = %.5f\" % (len(exp_replay), agent.epsilon))\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"mean reward per game\")\n",
        "        plt.plot(mean_rw_history)\n",
        "        plt.grid()\n",
        "        assert not np.isnan(loss_t)\n",
        "        plt.figure(figsize=[40, 4])\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"TD loss history (moving average)\")\n",
        "        plt.plot(pandas.ewma(np.array(td_loss_history), span=100, min_periods=100))\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJkfZ1IVeon3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "agent.epsilon = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lSz1pY6_W-BW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym.wrappers\n",
        "env_monitor = gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True)\n",
        "sessions = [evaluate(env_monitor, agent, n_games=1) for _ in range(100)]\n",
        "env_monitor.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8ovvG4Rehqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# show video\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = list(\n",
        "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1])) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}